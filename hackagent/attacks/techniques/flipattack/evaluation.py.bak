# Copyright 2025 - AI4I. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
FlipAttack evaluation module.

Evaluates attack success using both dictionary-based and LLM judge methods
from the original FlipAttack core implementation.

Result Tracking:
    Uses Tracker (passed via config["_tracker"]) to add evaluation traces
    per goal and sync evaluation status to server.
"""

import logging
from typing import TYPE_CHECKING, Any, Dict, List, Optional

from hackagent.attacks.evaluator import EVALUATOR_MAP, sync_evaluation_to_server
from hackagent.attacks.techniques.advprefix.config import EvaluatorConfig
from hackagent.client import AuthenticatedClient
from hackagent.router.types import AgentTypeEnum

if TYPE_CHECKING:
    from hackagent.router.tracking import Tracker

JUDGE_TYPE_LABELS = {
    "jailbreakbench": "JailbreakBench",
    "harmbench": "HarmBench",
    "nuanced": "Nuanced",
}


def _infer_judge_type(identifier: str) -> str:
    identifier_lower = identifier.lower() if identifier else ""
    if "harmbench" in identifier_lower:
        return "harmbench"
    if "nuanced" in identifier_lower:
        return "nuanced"
    if "jailbreak" in identifier_lower:
        return "jailbreakbench"
    return "jailbreakbench"


def _build_prompt_prefix(item: Dict[str, Any]) -> str:
    full_prompt = item.get("full_prompt")
    if full_prompt:
        return str(full_prompt)

    parts = [item.get("system_prompt", ""), item.get("user_prompt", "")]
    return "\n\n".join([p for p in parts if p]).strip()



def execute(
    input_data: List[Dict],
    config: Dict[str, Any],
    client: AuthenticatedClient,
    logger: logging.Logger,
) -> List[Dict]:
    """
    Evaluate attack responses using HackAgent judge evaluators.

    Uses judge config from top-level keys (judges, batch_size_judge, etc.)
    with backward-compatible fallbacks to flipattack_params.

    Args:
        input_data: List of dicts from generation step (with responses)
        config: Configuration dictionary
        client: Authenticated client for judge API calls
        logger: Logger instance

    Returns:
        Same list with added evaluation fields (success, judge scores, etc.)
    """
    fa_params = config.get("flipattack_params", {})
    judges_config = config.get("judges")
    judge_config: Dict[str, Any] = {}
    if isinstance(judges_config, list) and judges_config:
        judge_config = judges_config[0] or {}

    judge_model = (
        judge_config.get("identifier")
        or fa_params.get("judge")
        or "gpt-4-0613"
    )
    if not judge_model:
        logger.error("Judge model identifier is missing")
        return input_data

    tracker: Optional["Tracker"] = config.get("_tracker")
    tracking_client = config.get("_client")

    logger.info(f"Evaluating {len(input_data)} responses with judge={judge_model}...")

    if tracker:
        logger.info("ðŸ“Š Evaluation tracking via Tracker enabled")

    judge_type = (
        judge_config.get("evaluator_type")
        or judge_config.get("type")
        or fa_params.get("judge_type")
        or _infer_judge_type(judge_model)
    )
    evaluator_class = EVALUATOR_MAP.get(judge_type)
    if not evaluator_class:
        logger.error(f"Unknown judge type '{judge_type}'")
        return input_data

    agent_type_value = judge_config.get("agent_type") or "OPENAI_SDK"
    if isinstance(agent_type_value, AgentTypeEnum):
        agent_type = agent_type_value
    else:
        try:
            agent_type = AgentTypeEnum(str(agent_type_value).upper())
        except ValueError:
            logger.warning(
                f"Invalid judge_agent_type '{agent_type_value}', defaulting to OPENAI_SDK"
            )
            agent_type = AgentTypeEnum.OPENAI_SDK

    judge_metadata = dict(fa_params.get("judge_agent_metadata", {}) or {})
    judge_metadata.update(judge_config.get("agent_metadata", {}) or {})

    judge_api_key = (
        judge_config.get("api_key")
        or judge_config.get("api_key_env")
        or fa_params.get("judge_api_key")
        or fa_params.get("judge_api_key_env")
    )
    if judge_api_key:
        judge_metadata["api_key"] = judge_api_key

    evaluator_config = EvaluatorConfig(
        agent_name=(
            judge_config.get("agent_name")
            or fa_params.get("judge_agent_name")
            or f"flipattack-judge-{judge_type}"
        ),
        agent_type=agent_type,
        model_id=judge_model,
        agent_endpoint=judge_config.get("endpoint") or fa_params.get("judge_endpoint"),
        agent_metadata=judge_metadata,
        batch_size=config.get("batch_size_judge") or fa_params.get("judge_batch_size", 1),
        max_new_tokens_eval=config.get("max_new_tokens_eval")
        or fa_params.get("judge_max_new_tokens_eval", 256),
        filter_len=config.get("filter_len") or fa_params.get("judge_filter_len", 10),
        request_timeout=config.get("judge_request_timeout")
        or fa_params.get("judge_request_timeout", 120),
        temperature=config.get("judge_temperature")
        or fa_params.get("judge_temperature", 0.0),
        max_judge_retries=config.get("max_judge_retries")
        or fa_params.get("judge_max_retries", 1),
    )

    evaluator = evaluator_class(
        client=client,
        config=evaluator_config,
        run_id=config.get("_run_id"),
        tracking_client=tracking_client,
        tracker=tracker,
    )

    eval_column = evaluator_class.eval_column
    explanation_column = evaluator_class.explanation_column
    label = JUDGE_TYPE_LABELS.get(judge_type, judge_type)

    eval_rows: List[Dict[str, Any]] = []

    for idx, item in enumerate(input_data):
        logger.info(f"Evaluating response {idx+1}/{len(input_data)}")

        if item.get("error"):
            item["judge_score"] = 0
            item["judge_success"] = False
            item["success"] = False
            item["best_score"] = 0.0
            item["evaluation_notes"] = f"Execution error: {item['error']}"
            continue

        eval_rows.append(
            {
                "goal": item.get("goal", ""),
                "prefix": _build_prompt_prefix(item),
                "completion": item.get("response", "") or "",
            }
        )

    if eval_rows:
        try:
            evaluated_rows = evaluator.evaluate(eval_rows)
        except Exception as e:
            logger.error(f"Judge evaluation failed: {e}")
            evaluated_rows = []

        lookup: Dict[tuple, Dict[str, Any]] = {}
        for row in evaluated_rows:
            key = (
                str(row.get("goal") or ""),
                str(row.get("prefix") or ""),
                str(row.get("completion") or ""),
            )
            lookup[key] = row

        for idx, item in enumerate(input_data):
            if item.get("error"):
                continue

            key = (
                str(item.get("goal") or ""),
                str(_build_prompt_prefix(item) or ""),
                str(item.get("response") or ""),
            )
            merged = lookup.get(key, {})
            item[eval_column] = merged.get(eval_column, 0)
            item[explanation_column] = merged.get(
                explanation_column, "No judge output"
            )

            eval_value = item.get(eval_column, 0)
            try:
                eval_value = int(eval_value)
            except (TypeError, ValueError):
                eval_value = 0

            item["judge_score"] = eval_value
            item["judge_success"] = eval_value == 1
            item["success"] = item["judge_success"]
            item["best_score"] = float(eval_value)

            item["evaluation_notes"] = (
                f"{label}: {eval_value} | {item.get(explanation_column, '')}"
            )

            if tracker:
                goal_ctx = tracker.get_goal_context(idx)
                if goal_ctx:
                    tracker.add_evaluation_trace(
                        ctx=goal_ctx,
                        evaluation_result={
                            eval_column: eval_value,
                            "judge_model": judge_model,
                            "success": item["success"],
                        },
                        score=item["best_score"],
                        explanation=item["evaluation_notes"],
                        evaluator_name=f"flipattack_eval_{judge_type}",
                    )

    sync_evaluation_to_server(
        evaluated_data=input_data,
        client=tracking_client,
        logger=logger,
        judge_keys=[
            {
                "key": eval_column,
                "explanation": explanation_column,
                "label": label,
            }
        ],
    )

    total = len(input_data)
    judge_success = sum(1 for x in input_data if x.get("judge_success", False))
    overall_success = sum(1 for x in input_data if x.get("success", False))

    logger.info(
        f"ASR-{judge_model}: {judge_success}/{total} ({judge_success/total*100:.1f}%)"
    )
    logger.info(
        f"ASR-Overall: {overall_success}/{total} ({overall_success/total*100:.1f}%)"
    )

    return input_data

